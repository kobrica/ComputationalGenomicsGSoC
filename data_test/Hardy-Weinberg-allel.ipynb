{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-allel 1.2.1\n"
     ]
    }
   ],
   "source": [
    "### 1\n",
    "import numpy as np # A module for handling numerical arrays \n",
    "import os # A module for interacting with your operating system (e.g., listing directories)\n",
    "import gzip # A module to handle gzipped files\n",
    "import scipy # A module with scientific functions (such as statistical tests)\n",
    "import pandas # A module that handles general data arrays, in a way that is similar to R\n",
    "import matplotlib as mpl # A module for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns # A module to plot prettier plots\n",
    "sns.set_style('white')\n",
    "sns.set_style('ticks')\n",
    "sns.set_context('notebook')\n",
    "import h5py # A module for interacting with hdf5 data format.\n",
    "import allel; print('scikit-allel', allel.__version__) # A module to handle genetic data\n",
    "\n",
    "# Check version of scikit-allel -- this notebook may not work with older versions\n",
    "from distutils.version import LooseVersion # I import this module just to check that we have the\n",
    "                                           # correct version of allel -- otherwise things\n",
    "                                           # will fail\n",
    "assert LooseVersion(allel.__version__) >=  LooseVersion('1.2.0'), \\\n",
    "                \"This notebook may not work with this version of scikit-allel, requires 1.2.0\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an ipython notebook. Lectures about Python, useful both for beginners and experts, can be found at http://scipy-lectures.github.io.\n",
    "\n",
    "I recommend installing the [Anaconda](https://store.continuum.io/cshop/academicanaconda) distribution. Make sure not to pay for it! Click Anaconda Academic License; it should be free for those with academic e-mail addresses. \n",
    "\n",
    "\n",
    "Open the notebook by (1) copying this file into a directory, (2) in that directory typing\n",
    "ipython notebook\n",
    "and (3) selecting the notebook.\n",
    "\n",
    "This notebook uses the scikit-allel package. Follow instructions [here](https://anaconda.org/conda-forge/scikit-allel) to install it. \n",
    "\n",
    "Note: if you have an older version of anaconda installed, you should start by updating anaconda. \n",
    "\n",
    "\n",
    "# The Hardy-Weinberg principle\n",
    "In this exercise, we will warm up by manipulating some data from the 1000 Genomes Project and explore one of the most widely-used population genetics concepts, the Hardy-Weinberg principle. \n",
    "\n",
    "## Motivation\n",
    "If we want to be doing population genetics, we'll have to come up with a definition of what a \"population\" is. We already have an idea of what a diploid species is: it's a set of individuals that can reproduce together. Within a species, we may find multiple groups whose individuals *tend* to reproduce together, because biology, geography, or cultural factors favor matings within the group. This tendency can increase the genetic similarity of individuals within the groups. \n",
    "We'll say that a *population* is a group whose individuals reproduce preferentially with each other because of an inheritable feature. The inheritable feature is typically not genetic: it can involve geographic factors (eg, Peoples of the Americas), as well as political (e.g., Canadians), cultural (e.g., French Canadians), or a mix of all these factors (e.g., Montreal Canadians fans). None of these groups are in reproductive isolation, but individuals are statistically more likely to reproduce with members of their group, and so they form populations in the sense of population genetics. But it should be clear that not all these groups are useful to study from a genetics perspective. \n",
    "\n",
    "This brings up the question of why we need to define populations in the first place. In short, we define populations to help us build models of genetic diversity and evolution, when we think that each population can be described more simply than the whole species. For example, we often assume that indiviudals within a population behave in a relatively uniform manner. A perfectly uniform population never truly exists, and all our models are vast simplifications, but in many cases these simple models are sufficient to understand the biology that we care about. \n",
    "\n",
    "A particularly simple population is one where partners choose each other entirely randomly among all individuals. When that happens, the mathematical modelling gets much easier, because we don't have to model all the individual idiosyncracies. This is extremely convenient, and the random mating assumption plays a a central role in population genetics. The first thing we might want to do when looking at genetic data, therefore, is to check whether we can get away with this random mating assumption.\n",
    "\n",
    "Conveniently, the random mating assumption makes a very clear prediction about the genetic diversity in a population: the chromosomes inherited from each parent should be no more similar, nor dissimilar, than any two chromosomes sampled at random in the population. Therefore, if neutral alleles $a$ and $A$ segregate in a parental population, with frequencies $p_a+p_A=1$, the probability of genotypes in the descendants is \n",
    "\n",
    "$p_{AA}=p_A^2$, \n",
    "\n",
    "$p_{aA}=2 p_ap_A,$ \n",
    "\n",
    "$p_{aa}=p_ap_a$. \n",
    "\n",
    "You can verify that the expected proportion of $A$ alleles in the second generation is just  $p_A^{offspring}=p_{AA}+\\frac{p_{aA}}{2}=p_A$. Therefore, both the expected frequencies of alleles and of genotypes are conserved over time--this is the Hardy-Weinberg principle. \n",
    "\n",
    "Even without looking at the parental population, we can sometimes see that the genotype distribution does not follow the Hardy-Weinberg principle. If the counts of alleles in a population are:\n",
    "\n",
    "\n",
    "$\\#_{AA}=100$\n",
    "\n",
    "$\\#_{aA}=0$\n",
    "\n",
    "$\\#_{aa}=100,$\n",
    "\n",
    "we know that it's unlikely to have occurred in a randomly mating population--we can't find a $p_a$ and $p_A$ that match the data. If the data is consistent with the HW principle, we say that the allele is at Hardy-Weinberg equilibrium. We'll explore a few situations where Hardy-Weinberg equilibrium is not respected, but let's first look at some data and see what we find.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You can download the data file at either:\n",
    "\n",
    "ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/\n",
    "or\n",
    "ftp://ftp-trace.ncbi.nih.gov/1000genomes/ftp/release/20130502/\n",
    "\n",
    "Download \"ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz\" to your computer and take note of where you put it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/1000G/ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9305559045fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvcf_file_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"##\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Comment lines in vcf files start with ##. This skips comment lines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# The first line after the comment lines is the header line, starting w/ #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/gzip.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mgz_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"write\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/1000G/ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz'"
     ]
    }
   ],
   "source": [
    "### 2\n",
    "vcf_file_name = 'ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz'\n",
    "vcf_file_path = os.path.join('..',  'data', '1000G', vcf_file_name) # Change this path as \n",
    "                                                                    # necessary!\n",
    "\n",
    "# I used a relative path here, because I stored the data file in a directory close to the \n",
    "# notebooks. To find the file I just need to go up one directory ('..'), then go in the \n",
    "# \"data/1000G\" directory to use an absolute path, I would write something like\n",
    "# vcf_file = os.path.join(os.sep,'Users','simon', 'data', '1000G', vcf_file_name) \n",
    "                                                            \n",
    "import gzip \n",
    "for line in gzip.open(vcf_file_path,'rt'):\n",
    "    if not line.startswith(\"##\"): # Comment lines in vcf files start with ##. This skips comment lines\n",
    "        print('\\t'.join(line.split()[0:10])) # The first line after the comment lines is the header line, starting w/ #\n",
    "                                             # It gives information about what data is contained in each column.\n",
    "                                             \n",
    "    if not line.startswith(\"#\"): # After the header, we find data lines, which do not start with #\n",
    "                                 # We stop after we have read one such line  \n",
    "        break;\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not familiar with the vcf format, read about it here: http://samtools.github.io/hts-specs/VCFv4.2.pdf. It is a very commonly used format to present sequencing data for occurence_countiple individuals.\n",
    "\n",
    "When we read the vcf file, we are reading lines of text. To analyze the data, we would need to parse the file, that is, tell python which character on each line corresponds to a position, to a genotype, to a chomosome number, etc. We would need to do this in a computationally efficient manner, because reading the file itelf is computationally demanding. And we would need to handle all kinds of possible problems with the data, such as missing genotypes, multiallelic SNPs, etc.\n",
    "\n",
    "As is often the case in python, someone has already solved that problem for us and made it available in the scikit-allel package. \n",
    "Hopefully you were able to install the scikit-allel package -- if you did, you just saved yourself hours of pain.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step will be to convert the vcf file in hdf5 format used by scikit-allel. This format takes a bit more space on disk than the compressed vcf format, but it's a lot easier (and faster) to access specific subsets of the data. It will take a few minutes to convert the file the first time we run this, but afterwards things will be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3\n",
    "h5_file_name = \"chr22.h5\"\n",
    "h5_file_path =  os.path.join('..', 'data', '1000G', h5_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to read the hdf5 file. If we don't find it, we'll get an OSError. In that case, we'll create the file from the vcf. This took about 6 minutes on my computer.\n",
    "\n",
    "If you rerun the notebook a second time, the hdf5 file should exist and this should run much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4\n",
    "try:\n",
    "    callset = h5py.File(h5_file_path, mode='r')\n",
    "except OSError:\n",
    "    allel.vcf_to_hdf5(vcf_file_path, h5_file_path, fields='*', overwrite=True)\n",
    "    callset = h5py.File(h5_file_path, mode='r')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The allel package uses a variety of data structures to encode the data to save on memory and allow for fast computation. This may cause a steep learning curve, but it will hopefully pay off in the end.  \n",
    "\n",
    "The 'callset' variable is a dictionary that contains the different types of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5\n",
    "list(callset.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadly speaking, we have genotypes stored in 'calldata', meta-information about variants in 'variants', and information about samples IDs in 'samples': "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6\n",
    "callset['samples']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information about samples is stored as an HDF5 dataset, with 2504 elements (that is, the number of samples).  HDF5 arrays are memory-efficient because they don't load the data in memory unless it is needed. This is why the output is only giving us general information about the dataset, but is not printing the actual data. If we want to see the data, we can ask for specific elements using the array 'slice' notation. This will produce a regular in-memory array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7\n",
    "callset['samples'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hdf5 format is hierarchical: whereas the 'sample' category only contained one type of data, the 'calldata' category has more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8\n",
    "callset['calldata']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "callset['calldata'] is an hdf5 'group', rather than a dataset. This is also stored as a dictionary so we can look at its keys to see the sub-categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 9\n",
    "callset['calldata'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this is still slightly annoying: when we ask for the list of keys, the ouput is just a new data structure ('KeyView'), but doesn't show us the data itslef. Once again, this is designed to save memory. If we want to see the contents, we have to convert it to an in-memory format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 10\n",
    "list(callset['calldata'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The genotypes are stored under the key 'calldata/GT'. We could refer to the data itself as callset['calldata']['GT'], or, more simply, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 11\n",
    "callset['calldata/GT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that the array has dimension (1103547, 2504, 2). We have:\n",
    "\n",
    "1 103 547 snps,\n",
    "\n",
    "2504 individuals,\n",
    "\n",
    "2 genotypes per snp and per individual. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is stored as an hdf5 dataset. Because genotype data is where most of the analysis is taking place, scikit-allel has a specific storage format for genotyping data that is  similar to a numpy array, but has additional properties convenient for genotype data. For example, it has a function to count the number of distinct alleles for a given snp (this takes about a minute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 12\n",
    "genotypes = allel.GenotypeChunkedArray(callset['calldata/GT'])\n",
    "allele_counts = genotypes.count_alleles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the matrix of allele_counts, we also have methods to identify whether a site is biallelic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 13\n",
    "is_biallelic = allele_counts.is_biallelic_01()\n",
    "is_biallelic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll discard the non-biallelic sites, because they make everything so messy. When discarding sites, it's always a good idea to count how many things we are discarding. We'll do this a fair amount, so we might as well define a function that does this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 14\n",
    "def report_filter(filter_vector):\n",
    "    kept = np.sum(filter_vector)\n",
    "    length = filter_vector.shape[0]\n",
    "    print(\"kept %d out of %d. Ratio kept: %2.2f \" % (kept, length, kept/length))\n",
    "\n",
    "report_filter(is_biallelic)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can extract only the snps that are biallelic. We'll want to keep track of the SNP positions, the genotypes, and the allele counts. The 'compress' function allows us to extract the part of the array for which a condition evaluated to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 15\n",
    "print(\"original\")\n",
    "print(callset['variants']['ALT'][:11])\n",
    "print(\"\\n compressed\")\n",
    "print(np.compress(is_biallelic,callset['variants']['ALT'], axis = 0)[:11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for the other arrays we will need. This also takes a couple of seconds because we need to make copies of the arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 16\n",
    "genotypes_biallelic = genotypes.compress(is_biallelic)\n",
    "allele_counts_biallelic_all_alleles = allele_counts.compress(is_biallelic)\n",
    "pos_biallelic = callset['variants/POS'][:].compress(is_biallelic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the resulting genotype and genotype count arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 17\n",
    "genotypes_biallelic[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 18\n",
    "allele_counts_biallelic_all_alleles[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has plenty of columns because we used to have multiallelic variants. We can compress to extract columns as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 19\n",
    "relevant_column = np.array([False] * allele_counts_biallelic_all_alleles.shape[1])\n",
    "relevant_column[0:2] = True\n",
    "allele_counts_biallelic = allele_counts_biallelic_all_alleles.compress(relevant_column, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 20\n",
    "allele_counts_biallelic[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the allele counts, it is easy to compute the frequency of the alternate allele:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 21\n",
    "alt_allele_freqs = allele_counts_biallelic[:,1] / allele_counts_biallelic[:].sum(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we don't really care about whether a genotype is listed as 1/0 or 0/1 -- we only care about whether we have 0, 1, or 2 copies of the alternate allele. We can convert genotype data into integer value alternate allele count using to_n_alt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 22\n",
    "\n",
    "genotypes_012 = genotypes_biallelic.to_n_alt(fill=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for Hardy-Weinberg\n",
    "There are many distributions of genotype counts that can produce the same number of reference and alternate alleles. \n",
    "If I have 8 copies of the A allele and 8 copies of the a allele divided among 8 diploid individuals, we could have the following configuration:\n",
    "\n",
    "AA AA aA Aa Aa Aa aa aa\n",
    "\n",
    "which obeys the Hardy-Weinberg proportions perfectly.\n",
    "\n",
    "\n",
    "$p_{A}=\\frac{1}{2}$\n",
    "\n",
    "$p_{a}=\\frac{1}{2}$\n",
    "\n",
    "$p_{aa}=\\frac{1}{4} = p_a^2$\n",
    "\n",
    "$p_{AA}=\\frac{1}{4} = p_A^2$\n",
    "\n",
    "$p_{aA}=\\frac{1}{2} = 2 p_a p_A$\n",
    "\n",
    "\n",
    "\n",
    "Let's first code functions that will compute the HW genotype proportions:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 23\n",
    "def proportion_hom_ref(frequencies):\n",
    "    \"\"\"computes the proportion of homozygote reference genotypes at a given alternate allele \n",
    "    frequency. This should work when frequency is a number or an array of frequencies.\n",
    "    The output should have the same dimension as the input.\n",
    "    \"\"\"\n",
    "    return (1-frequencies)**2 #s/n.*/n .../\n",
    "\n",
    "def proportion_het(frequencies):\n",
    "    \"\"\"computes the proportion of heterozygote genotypes at a given aleternate allele frequency.\n",
    "    This should work when frequency is a number or a vector of frequency.\n",
    "    The output should have the same dimension as the input.\n",
    "    \"\"\"\n",
    "    return 2*frequencies*(1-frequencies) #s/n.*/n .../\n",
    "\n",
    "\n",
    "\n",
    "def proportion_hom_alt(frequencies):\n",
    "    \"\"\"computes the proportion of homozygote alternate genotypes at a given alternate allele frequency..\n",
    "    This should work when frequency is a number or a vector of frequency.\n",
    "    The output should have the same dimension as the input.\n",
    "    \"\"\"\n",
    "    return frequencies**2 #s/n.*/n .../\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are expectations under a theoretical model. The next step is to look at the data to see if the model describes the data well. The first step is to count the genotype data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 24\n",
    "homalt_counts = (genotypes_012==2).sum(axis = 1)\n",
    "het_counts = (genotypes_012==1).sum(axis = 1)\n",
    "homref_counts = (genotypes_012==0).sum(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next two code boxes, we'll compute the number of aa, aA, and AA at each site, and see how these proportions compare to the expectation. The resulting graph is called a \"De finetti\" diagram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 25\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "####\n",
    "# First plot the Hardy-Weinberg expectation curves. These plots shouldn't use the data at all -- we'll just generate a\n",
    "# list of frequencies, p_range, and you'll plot the expected proportion of each genotype count for each frequency.   \n",
    "\n",
    "\n",
    "\n",
    "number_of_genotypes = homref_counts + het_counts + homalt_counts \n",
    "\n",
    "###########\n",
    "# Then plot the observed proportion of each genotype versus the allele frequency. \n",
    "# Each snp is represented by three snps.  \n",
    "plt.plot(alt_allele_freqs, np.array(homref_counts) / number_of_genotypes, 'ro', label = \"homozygote reference\" )\n",
    "plt.plot(alt_allele_freqs, np.array(het_counts) / number_of_genotypes, 'o', color = 'orange', label = \"heterozygote\" )\n",
    "plt.plot(alt_allele_freqs, np.array(homalt_counts) / number_of_genotypes, 'bo', label = \"homozygote alternate\")\n",
    "plt.legend(loc='upper center')\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel(\"alternate allele frequency\")\n",
    "plt.ylabel(\"proportion of genotypes\")\n",
    "\n",
    "\n",
    "p_range = np.arange(0,1,.01) # A sorted list of frequencies\n",
    "\n",
    "\n",
    "plt.plot(p_range, proportion_hom_ref(p_range), 'crimson' ) # homozygote reference expectations \n",
    "plt.plot(p_range, proportion_het(p_range),  color = 'orangered' ) # heterozygote expectations \n",
    "plt.plot(p_range, proportion_hom_alt(p_range), 'aqua') # homozygote alternate expectations \n",
    "plt.title(\"De finetti diagram for 1000 Genomes chromosome 22\")\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we don't have HW equilibrium, it suggests that the two alleles are not inherited independently--either the parents tend to be more related (if we have an excess of homozygotes), or they tend to be less related (if we have an excess of heterozygotes). The former is much more common! \n",
    "\n",
    "However, HW disequilibrium can mean something much less biological--it can indicate that our sequencer is biased and produces too many or too few homozygotes. This is also something that we would like to know, and detecting these issues is one of the main applications of the Hardy-Weinberg principle. \n",
    "\n",
    "Even if there is no such problem, we don't expect the Hardy-Weinberg proportions to hold exactly because the number of individuals is finite. When you look at the scatter of points on your figure, it's hard to tell which points truly deviate from HW, and which are complete outliers. \n",
    "\n",
    "To test each site for Hardy-Weinberg equilibrium, we need to build a statistical test. Our null model is that the two alleles of each individual are drawn with probability $p_a$ and $p_A=1-p_a$. Our observations are the numbers $N_{aa}$, $N_{aA}$ and $N_{AA}$ of each genotype, with $N_{aa}+N_{aA}+N_{AA}=N$. We don't quite know the probabilty $p_a$, but we can estimate it from the data:\n",
    "\n",
    "$$\\hat p_a=\\frac{2N_{aa}+N_{aA}}{2N}=1-\\hat p_A.$$\n",
    "We then expect to have \n",
    "\n",
    "$N_{aa}\\simeq e_{aa}=N \\hat p_a^2$\n",
    "\n",
    "$N_{AA}\\simeq e_{AA}= N \\hat p_{A}^2$\n",
    "\n",
    "$N_{Aa,~aA} \\simeq e_{Aa} = 2 N \\hat p_{a} \\hat p_{A}.$\n",
    "\n",
    "A popular way to test the significance of departures from this distribution is to consider the test statistic\n",
    "\n",
    "$X^2= \\frac{(N_{aa}-e_{aa})^2}{e_{aa}}+ \\frac{(N_{Aa}-e_{Aa})^2}{e_{Aa}}+ \\frac{(N_{AA}-e_{AA})^2}{e_{AA}}.$\n",
    "\n",
    "Intuitively, this statistic measures the departure from our null model. Perhaps less intuitively, it follows a $\\chi^2_1$ (a chi-squared distribution with one degree of freedom) if the null model is true and we have enough data. \n",
    "\n",
    "\n",
    "\"Enough data\" means that we should expect at least a few counts in each category, so it would make sense to request at least 5 counts in both aa and AA genotypes. I.e., $e_{aa}=p_a^2*N>5$ leading to $p_a>\\sqrt{5/N},$ and similarly $p_A>\\sqrt{5/N}.$\n",
    "\n",
    "**This would be a good place to complete the chisquared notebook, which explains why this statistical test is appropriate**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*Additional optional reading about Hardy-Weinberg testing* \n",
    "http://link.springer.com/protocol/10.1007%2F978-1-61779-555-8_6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform a chi-squared test, we first need to compute the expectations for each genotype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 26\n",
    "\n",
    "        \n",
    "        \n",
    "hom_ref_expectation = allele_counts_biallelic[:,0]**2 / (4.*np.array(number_of_genotypes)) # the expected number of homozygote references #s/=.*$/= .../ \n",
    "het_expectation = allele_counts_biallelic[:,0] * allele_counts_biallelic[:,1] / (2.*np.array(number_of_genotypes))  # the expected number of hets #s/=.*$/= .../ \n",
    "hom_alt_expectation = allele_counts_biallelic[:,1]**2 / (4.*np.array(number_of_genotypes) ) # the expected number of homozygote nonreferences #s/=.*$/= .../ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 27\n",
    "expectations = np.vstack([hom_ref_expectation,het_expectation, \n",
    "                                                  hom_alt_expectation] ).transpose()\n",
    "\n",
    "\n",
    "observations = np.vstack([homref_counts, het_counts, \n",
    "                                                  homalt_counts] ).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 28\n",
    "print(\"expectations for snp 0\")\n",
    "print(expectations[0,:])\n",
    "print(\"observations for snp 0\")\n",
    "print(np.array(observations[0,:]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing the chi-squared test is now very straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 29\n",
    "hw_test_results = scipy.stats.chisquare(observations, expectations, axis = 1, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 30\n",
    "hw_p_values = hw_test_results[1]\n",
    "excess_het = het_counts/expectations[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 31\n",
    "n_inspect = 30\n",
    "for i in range(n_inspect):\n",
    "    print(\"genotypes\\taa\\taA\\tAA\")\n",
    "    print(\"observed\\t%d\\t%d\\t%d\" % (homref_counts[i], het_counts[i], homalt_counts[i]))\n",
    "    print(\"expected\\t%2.1f\\t%2.1f\\t%2.1f\" % (expectations[i,0], expectations[i,1], expectations[i,2]))\n",
    "    print(\"chi-squared p-value,\\t\", hw_p_values[i],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something here should make you feel a bit uncomfortable - we don't fulfil the conditions under which the chi-square test is accurate. So we'd want to restrict the search to places where expected counts in each category are at least 5. \n",
    "\n",
    "*Discussion question*\n",
    "\n",
    "Does it mater if the number of *observations* is less than 5? Why?\n",
    "\n",
    "\n",
    "So we'll now only consider snps that have enough observations to justify the chi-square test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 32\n",
    "is_common = expectations.min(axis=1)>5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 33\n",
    "n_inspect = 100\n",
    "for i in range(n_inspect):\n",
    "    if is_common[i]:\n",
    "        print(\"genotypes\\taa\\taA\\tAA\")\n",
    "        print(\"observed\\t%d\\t%d\\t%d\" % (homref_counts[i], het_counts[i], homalt_counts[i]))\n",
    "        print(\"expected\\t%2.1f\\t%2.1f\\t%2.1f\" % (expectations[i,0], expectations[i,1], expectations[i,2]))\n",
    "        print(\"HW p-value,\\t\", hw_p_values[i],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of these sites are significantly departing from the Hardy-Weinberg assumptions. Most of them have too many homozygotes, as we would expect under inbreeding, but a few sites have way too many hets. What do you think is going on?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the snps obey our null model, the distribution of p-values should be uniformly distributed between 0 and 1.\n",
    "\n",
    "*Convince yourself that this expectation should hold true, mathematically or by simulating it*\n",
    "\n",
    "We can inspect this visually. First we'll extract the p-values for positions where the chi-square test is applicable.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 34\n",
    "common_p_values  = hw_p_values.compress(is_common)\n",
    "common_positions = pos_biallelic.compress(is_common) \n",
    "common_excess_het = excess_het.compress(is_common) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we plot the distribution of p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 35\n",
    "plt.hist(common_p_values, bins=100);\n",
    "plt.xlabel(\"p_values\")\n",
    "plt.ylabel(\"number of observations\")\n",
    "plt.title(\"distribution of chi-squared p-values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you interpret this figure? \n",
    "\n",
    "We'd now like to distinguish between sites who fail Hardy-Weingberg because of technical reasons from those who fail because of biological or population genetic reasons. One thing to look for is whether SNPs that fail the test clump together: sequencing artifacts tend to come in batches, and if we find a few bad regions, we'll have a hint that the errors are likely to be technical.  \n",
    "\n",
    "Let's plot the excess of heterozygosity vs. genomic position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 36\n",
    "plt.plot(common_positions, common_excess_het[:],'o',markersize=1)\n",
    "plt.xlabel(\"genomic position\")\n",
    "plt.ylabel(\"excess heterozygosity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zooming in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 37\n",
    "plt.plot(common_positions[:10000], common_excess_het[:10000],'o',markersize=1)\n",
    "plt.xlabel(\"genomic position\")\n",
    "plt.ylabel(\"excess heterozygosity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be regions that behave poorly. These could be regions undergoing selection, but most likely they are regions of poor call quality. Because it's a large number of sites, it's a concern. Maybe we want to report those to the 1000 Genomes project?\n",
    "\n",
    "Before we do this, it would make sense to look at whether the problem was identified already. The 1000 genomes project has a \"mask\" file that tells users of the regions that should be considered trustworthy, and which should be considered with caution. \n",
    "\n",
    "This file is at:\n",
    "ftp://ftp-trace.ncbi.nih.gov/1000genomes/ftp/release/20130502/supporting/accessible_genome_masks/20141020.strict_mask.whole_genome.bed\n",
    "\n",
    "from the accompanying readme file:\n",
    "\n",
    "*The StrictMask directory uses a more stringent definition. This definition uses\n",
    "a narrower band for coverage, requiring that total coverage should be within 50%\n",
    "of the average, that no more than 0.1% of reads have mapping quality of zero, \n",
    "and that the average mapping quality for the position should be 56 or greater.\n",
    "This definition is quite stringent and focuses on the most unique regions of the \n",
    "genome. The average total depth of coverage across Phase 3 samples is 17920. Thus, sites\n",
    "with a depth of coverage of <8960 or >26880 were excluded.\n",
    "Overall, this strict mask results in about 6.8% of bases marked N, 1.1% marked\n",
    "L, 0.5% marked H, 16.8% marked Z, and 3.1% marked Q. The remaining 71.7% of\n",
    "passed are marked passed (P) - corresponding to 76.9% of the non-N bases.*\n",
    "\n",
    "The bed file format is often used to describe genomic regions; specifications can be found here: \n",
    "\n",
    "http://genome.ucsc.edu/FAQ/FAQformat.html\n",
    "There's a suite of tools to handle bed files, https://github.com/arq5x/bedtools2. But here we'll want to manipulate it ourselves, so we'll write our own parser. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 38\n",
    "bed_file = os.path.join('..', 'data', '1000G', '20141020.strict_mask.whole_genome.bed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 39\n",
    "mask_bed = pandas.read_csv(bed_file, sep='\\t', header=None)\n",
    "mask_bed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only care about the entries relevant to chromosome 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 40\n",
    "mask_bed = mask_bed.loc[mask_bed[0] == 'chr22']\n",
    "n_features = mask_bed.shape[0]\n",
    "print(\"number of features:\", n_features)\n",
    "mask_bed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .bed format has the particularity that the start coordinate is exclusive, and the end coordinates is exclusive, so that the first segment runs from 754451 to 754464, inclusively. This was decided so that lengths can be computed directly by taking the difference of the two columns. I disapprove of that convention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's overlay the two pieces of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 41\n",
    "starts = np.array(mask_bed[1]) + 1\n",
    "ends = np.array(mask_bed[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 42\n",
    "start_at_snp = 0\n",
    "n_snps_to_plot = 20000\n",
    "start_position = common_positions[0]\n",
    "end_position = common_positions[n_snps_to_plot]\n",
    "plt.plot(common_positions[:n_snps_to_plot], common_excess_het[:n_snps_to_plot],'o',markersize=1)\n",
    "plt.xlabel(\"genomic position\")\n",
    "plt.ylabel(\"excess heterozygosity\")\n",
    "for i in range(len(starts)):\n",
    "    feature_start = starts[i] \n",
    "    feature_end = ends[i]\n",
    "    if (feature_start < start_position and feature_end > start_position) \\\n",
    "    or ( start_position < feature_start < end_position ):\n",
    "        plt.plot([feature_start, feature_end], [1,1], 'r-', lw=2)\n",
    "plt.plot([feature_start, feature_end], [1,1],\n",
    "         'r-', lw=2, label=\"good regions\") # Add one more to produce a legend\n",
    "plt.xlim([start_position, end_position])\n",
    "plt.legend(loc = \"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 43\n",
    "in_mask = (common_positions<0) # Initialize with all False\n",
    "i = 0\n",
    "for _index, feature in mask_bed.iterrows():\n",
    "    i+=1\n",
    "    if i%10000 == 0:\n",
    "        print(\"processed\", i, \"features out of\", n_features)\n",
    "\n",
    "    start = feature[1]\n",
    "    end = feature[2]\n",
    "      \n",
    "    in_mask = np.logical_or(in_mask, np.logical_and(common_positions>=start, \n",
    "                                                    common_positions<=end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 44\n",
    "report_filter(in_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll refer to genotypes within the 'strict' bed file with the prefix 'strict_'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 45\n",
    "strict_positions = common_positions.compress(in_mask)\n",
    "strict_excess_het = common_excess_het.compress(in_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat the plotting, but only with the strict variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 46\n",
    "start_at_snp = 0\n",
    "n_snps_to_plot = 20000\n",
    "start_position = strict_positions[0]\n",
    "end_position = strict_positions[n_snps_to_plot]\n",
    "plt.plot(strict_positions[:n_snps_to_plot], strict_excess_het[:n_snps_to_plot],'o',markersize=1)\n",
    "plt.xlabel(\"genomic position\")\n",
    "plt.ylabel(\"excess heterozygosity\")\n",
    "for i in range(len(starts)):\n",
    "    feature_start = starts[i] \n",
    "    feature_end = ends[i]\n",
    "    if (feature_start < start_position and feature_end > start_position) \\\n",
    "    or ( start_position < feature_start < end_position ):\n",
    "        plt.plot([feature_start, feature_end], [1,1], 'r-', lw=2)\n",
    "plt.plot([feature_start, feature_end], [1,1],\n",
    "         'r-', lw=2, label=\"good regions\") # Add one more to produce a legend\n",
    "plt.xlim([start_position, end_position])\n",
    "plt.legend(loc = \"upper right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the De Finetti diagram again, but only on the strict snps, we need to extract some more information: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 47\n",
    "strict_homref_counts = homref_counts.compress(is_common).compress(in_mask) \n",
    "strict_het_counts = het_counts.compress(is_common).compress(in_mask)\n",
    "strict_homalt_counts = homalt_counts.compress(is_common).compress(in_mask)\n",
    "strict_alt_allele_freqs = alt_allele_freqs.compress(is_common).compress(in_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 48\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "####\n",
    "# First plot the Hardy-Weinberg expectation curves. These plots shouldn't use the data at all -- we'll just generate a\n",
    "# list of frequencies, p_range, and you'll plot the expected proportion of each genotype count for each frequency.   \n",
    "\n",
    "\n",
    "\n",
    "strict_number_of_genotypes = strict_homref_counts + strict_het_counts + strict_homalt_counts \n",
    "\n",
    "###########\n",
    "# Then plot the observed proportion of each genotype versus the allele frequency. \n",
    "# Each snp is represented by three snps.  \n",
    "plt.plot(strict_alt_allele_freqs, np.array(strict_homref_counts) / strict_number_of_genotypes, 'ro', label = \"homozygote reference\" )\n",
    "plt.plot(strict_alt_allele_freqs, np.array(strict_het_counts) / strict_number_of_genotypes, 'o', color = 'orange', label = \"heterozygote\" )\n",
    "plt.plot(strict_alt_allele_freqs, np.array(strict_homalt_counts) / strict_number_of_genotypes, 'bo', label = \"homozygote alternate\" )\n",
    "plt.legend(loc='upper center')\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel(\"alternate allele frequency\")\n",
    "plt.ylabel(\"proportion of genotypes\")\n",
    "\n",
    "\n",
    "p_range = np.arange(0,1,.01) # A sorted list of frequencies\n",
    "\n",
    "\n",
    "plt.plot(p_range, proportion_hom_ref(p_range), 'crimson' ) # homozygote reference expectations \n",
    "plt.plot(p_range, proportion_het(p_range),  color = 'orangered' ) # heterozygote expectations \n",
    "plt.plot(p_range, proportion_hom_alt(p_range), 'aqua' ) # homozygote alternate expectations \n",
    "\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things should look much better now! Do you think that alleles satisfy Hardy-Weinberg equilibrium? \n",
    "\n",
    "One way to check is to run again our statistical test and plot the distribution of p-values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 49\n",
    "strict_p_values = common_p_values.compress(in_mask)\n",
    "plt.hist(strict_p_values, bins=100);\n",
    "plt.xlabel(\"p_values\")\n",
    "plt.ylabel(\"number of observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still are enriched for departures from Hardy-Weinberg.  We can again plot the Hardy-Weinberg departures versus genomic positions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have a bit too few heterozygotes, but now we have fewer clumps of crummy data. \n",
    "\n",
    "Then we are finally left to explore our population genetic hypothesis--the idea that the excess of homozygotes is caused by population structure. \n",
    "\n",
    "One thing that we can do to verify this further is calculate the HW statistic within each population comprising the 1000 Genomes Project. \n",
    "\n",
    "The information about 1000 Genomes populations is [here](goo.gl/9Hb83M)\n",
    "(full path: \n",
    "ftp://ftp-trace.ncbi.nih.gov/1000genomes/ftp/release/20130502/integrated_call_samples_v3.20130502.ALL.panel)\n",
    "\n",
    "\n",
    "\\>head integrated_call_samples_v3.20130502.ALL.panel\n",
    "\n",
    "sample\tpop\tsuper_pop\tgender\t\t\n",
    "\n",
    "HG00096\tGBR\tEUR\tmale\n",
    "\n",
    "HG00097\tGBR\tEUR\tfemale\n",
    "\n",
    "HG00099\tGBR\tEUR\tfemale\n",
    "\n",
    "HG00100\tGBR\tEUR\tfemale\n",
    "\n",
    "HG00101\tGBR\tEUR\tmale\n",
    "\n",
    "HG00102\tGBR\tEUR\tfemale\n",
    "\n",
    "HG00103\tGBR\tEUR\tmale\n",
    "\n",
    "HG00105\tGBR\tEUR\tmale\n",
    "\n",
    "HG00106\tGBR\tEUR\tfemale\n",
    "\n",
    "This is a small file, so we can read it in memory. The pandas library contains many useful functions to read and parse files in table format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 50\n",
    "population_file_name = 'integrated_call_samples_v3.20130502.ALL.panel'\n",
    "population_file = os.path.join('..', 'data', '1000G', population_file_name)\n",
    "samples = pandas.read_csv(population_file, sep='\\t')\n",
    "samples.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count number of samples per population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 51\n",
    "samples['pop'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 52\n",
    "populations = np.array(samples['pop'].value_counts().keys())\n",
    "populations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find positions of samples within each population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 53\n",
    "# index samples using positions\n",
    "samples.reset_index(drop=True, inplace=True)\n",
    "samples.head()\n",
    "\n",
    "subpops = {\n",
    "    # for each population, get the list of samples that belong to the population\n",
    "    pop_iter: samples[samples['pop'] == pop_iter].index.tolist() for pop_iter in populations\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 54\n",
    "subpops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now extract genotypes within the stric mask so that we can compute Hardy-Weinberg properties within each population. The following step takes a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 55\n",
    "strict_genotypes = genotypes_biallelic.compress(is_common).compress(in_mask)\n",
    "strict_genotypes_012 = genotypes_012.compress(is_common).compress(in_mask)\n",
    "ac_subpops = strict_genotypes.count_alleles_subpops(subpops, max_allele=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is a bit of a hack. I wanted to use the .count_alleles_subpops function to count the number of homozygotes and heterozygotes within each sub-population. This was not supported, so I cheat by pretending that the genotypes 0,1, and 2 are actually three different alleles in a haploid individual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 56\n",
    "genotype_012_as_triallelic_haploid = allel.GenotypeArray(strict_genotypes_012[:].reshape(\n",
    "    [strict_genotypes_012.shape[0], strict_genotypes_012.shape[1], 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 57\n",
    "observed_counts = genotype_012_as_triallelic_haploid.count_alleles_subpops(subpops, max_allele=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected heterozygosity within a population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 58\n",
    "\n",
    "for i,pop in enumerate(populations):\n",
    "    #to reduce the plotting burden, don't plot unless freqency is above 5%\n",
    "    \n",
    "    freq = ac_subpops[pop][:,1]/ac_subpops[pop].sum(axis = 1)\n",
    "    iscommon = np.logical_and((freq>0.05), (freq<0.95))\n",
    "    freq_common = freq.compress(iscommon)\n",
    "    counts = observed_counts[pop].compress(iscommon)\n",
    "    tot_counts = counts.sum(axis = 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.plot(freq_common, counts[:,0]/ tot_counts, 'x', \n",
    "             color = 'r', markersize = 0.5 )\n",
    "    plt.plot(freq_common[:], counts[:,1]/ tot_counts[:], 'x', \n",
    "             color = 'orange', markersize = 0.5)\n",
    "    plt.plot(freq_common, counts[:,2]/ tot_counts,'x', \n",
    "             color = 'b', markersize = 0.5 )\n",
    "    plt.ylim([0,1])\n",
    "    plt.xlabel(\"alternate allele frequency\")\n",
    "    plt.ylabel(\"proportion of genotypes\")\n",
    "    \n",
    "    ####\n",
    "    # Now plot the theory values\n",
    "\n",
    "x_range = np.arange(0,1,.01) # A list of sorted values to help in plotting\n",
    "\n",
    "plt.plot(x_range, proportion_hom_ref(x_range), 'r', \n",
    "         label = \"homozygote reference\" ) \n",
    "plt.plot(x_range,  proportion_het(x_range),  color = 'orange', label = \"heterozygote\" ) \n",
    "plt.plot(x_range,  proportion_hom_alt(x_range), 'b',label = \"homozygote alternate\" ) \n",
    "plt.legend(loc = \"upper center\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the heterozygosity vs expectations within each population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 59\n",
    "het_expectations = {}\n",
    "excess_hets = {}\n",
    "excess_hets_ratio = {}\n",
    "het_count_cutoff = 0\n",
    "for population in populations:\n",
    "    allele_counts_subpop = ac_subpops[population]\n",
    "    number_of_genotypes_subpop = observed_counts[population].sum(axis=1)\n",
    "    het_expectation = allele_counts_subpop[:,0] * allele_counts_subpop[:,1] / (2.*np.array(\n",
    "        number_of_genotypes_subpop)) \n",
    "    filter_expectations = het_expectation > het_count_cutoff\n",
    "\n",
    "    het_expectations[population] = np.compress(filter_expectations, het_expectation)\n",
    "    excess_hets[population] = np.compress(filter_expectations, observed_counts[population][:,1]-het_expectation)\n",
    "    excess_hets_ratio[population] = np.compress(filter_expectations, observed_counts[population][:,1]) \\\n",
    "                                /np.compress(filter_expectations, het_expectation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 60\n",
    "plt.boxplot([list(excess_hets[pop])[:] for pop in populations], showmeans = True);\n",
    "plt.plot([0,len(populations)],[0,0],'k--')\n",
    "plt.xticks(np.arange(len(populations))+1.2,populations, rotation=90);\n",
    "plt.xlabel(\"population\")\n",
    "plt.ylabel(\"excess hets (absolute difference)\")\n",
    "plt.ylim([-5,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall counts of heterozygotes vs the number predicted by Hardy-Weinberg. We have a bit too many observed heterozygotes. I'm not sure why. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 61\n",
    "print(\"expected\", np.sum([het_expectations[pop][:].sum() for pop in populations]))\n",
    "print(\"observed\", np.sum([excess_hets[pop].sum() + het_expectations[pop][:].sum()for pop in populations]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 62\n",
    "plt.boxplot([list(excess_hets_ratio[pop])[:] for pop in populations], showmeans = True);\n",
    "plt.plot([0,len(populations)],[1,1],'k--')\n",
    "plt.xticks(np.arange(len(populations))+1.2,populations, rotation=90);\n",
    "plt.xlabel(\"population\")\n",
    "plt.ylabel(\"excess hets ratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compute the excess heterozygosity per individual rather than per per SNP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 63\n",
    "het_per_sample = (genotype_012_as_triallelic_haploid==1).sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each population, compute the expected heterozygosity per individual (this is the same for all individuals in the population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 64\n",
    "het_per_sample\n",
    "excess_het_per_sample_per_pop = {} \n",
    "total_observed_hets = 0\n",
    "total_expected_hets = 0\n",
    "for pop in populations:\n",
    "    expected_hets = het_expectations[pop].sum()/samples['pop'].value_counts()[pop]\n",
    "    observed_hets = het_per_sample[subpops[pop]]\n",
    "    excess_het_per_sample_per_pop[pop] = observed_hets - expected_hets\n",
    "    total_observed_hets += np.sum(het_per_sample[subpops[pop]])\n",
    "    total_expected_hets += het_expectations[pop].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 65\n",
    "_=plt.boxplot([excess_het_per_sample_per_pop[pop].reshape([samples['pop'].value_counts()[pop]]) \n",
    "             for pop in populations],showmeans = True)\n",
    "plt.xticks(np.arange(len(populations))+1.2,populations, rotation=90);\n",
    "plt.plot([0,len(populations)],[0,0],'k--')\n",
    "\n",
    "plt.xlabel(\"population\")\n",
    "plt.ylabel(\"excess hets\")\n",
    "plt.title(\"distribution of excess hets per individual\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By contrast to the full population case, there doesn't appear to be a strong excess of heterozygotes once populations have been split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we considered the entire population, there was clearly an excess of homozygotes, but here it's more subtle. The Q-Q plots should also look better:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a few questions to think about: \n",
    "\n",
    "* Are the 25 populations in the 1000 Genomes project truly \"populations\"? I.e., are they in Hardy-weinberg equilibrium?\n",
    "\n",
    "* The code we wrote is not very efficient--we were only able to analyze a tiny subset of one genome. How would you make it more efficient? It doesn't need to be a fancy programming trick. Try it out. \n",
    "\n",
    "* There are still regions that fail the HW test. What do you think is going on there? How would you go about figuring it out?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transitions and transversions\n",
    "\n",
    "Elevated Hardy-Weiberg disequilibrium for some variants can be an indicator of poor quality of the sequencing data. Here we discuss another measure of variant quality: the ratio of trnasitions to transversions. This should have been discussed in class -- otherwise, you can read explanations from [this scikit tutorial](http://alimanfoo.github.io/2016/06/10/scikit-allel-tour.html), which I used extensively in preparing these notes. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can only compute transitions/transversions for biallelic sites, so we first restrict to biallelic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 66\n",
    "refs = np.array(callset['variants']['REF']).compress(is_biallelic, axis=0).astype(str)\n",
    "alts = np.array(callset['variants']['ALT'])[:,0].compress(is_biallelic, axis=0).astype(str)\n",
    "mutations = np.char.add(refs, alts)\n",
    "mutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 67\n",
    "# This function is modified from http://alimanfoo.github.io/2016/06/10/scikit-allel-tour.html\n",
    "#\n",
    "def locate_transitions(x):\n",
    "    x = np.asarray(x)\n",
    "    return np.logical_or.reduce(((x == 'AG'), (x == 'GA') , (x == 'CT') , (x == 'TC')))\n",
    "is_ti = locate_transitions(mutations)\n",
    "is_ti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define a function that takes the vector of mutations and computes the proportion of transitions to transversions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 68\n",
    "# This function is copied from http://alimanfoo.github.io/2016/06/10/scikit-allel-tour.html\n",
    "\n",
    "def ti_tv(x):\n",
    "    if len(x) == 0:\n",
    "        return np.nan\n",
    "    is_ti = locate_transitions(x)\n",
    "    n_ti = np.count_nonzero(is_ti)\n",
    "    n_tv = np.count_nonzero(~is_ti)\n",
    "    if n_tv > 0:\n",
    "        return n_ti / n_tv\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 69\n",
    "print(\"ti_tv for all mutations\", ti_tv(mutations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 70\n",
    "common_mutations = mutations.compress(is_common)\n",
    "print(\"ti_tv for all mutations\", ti_tv(common_mutations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 71\n",
    "strict_mutations = common_mutations.compress(in_mask)\n",
    "print(\"ti_tv for all mutations\", ti_tv(strict_mutations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 72\n",
    "ti_tv(strict_mutations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 73\n",
    "rare_mutations = mutations.compress(1-is_common)\n",
    "print(\"ti_tv for rare mutations\", ti_tv(rare_mutations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, restricting the analysis to the strict mask increases the ti/tv ratio. This is consistent with the much higher rate of false positives in the low-quality regions. \n",
    "\n",
    "By contrast, the lower rate of transitions among common mutations was a surprise to me.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Simon Gravel, McGill University. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
