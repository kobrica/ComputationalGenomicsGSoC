{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an ipython notebook. Lectures about Python, useful both for beginners and experts, can be found at http://scipy-lectures.github.io.\n",
    "\n",
    "I recommend installing the [Anaconda](https://store.continuum.io/cshop/academicanaconda) distribution. Make sure not to pay for it! Click Anaconda Academic License; it should be free for those with academic e-mail addresses.\n",
    "\n",
    "\n",
    "Open the notebook by (1) copying this file into a directory, (2) in that directory typing\n",
    "ipython notebook\n",
    "and (3) selecting the notebook.\n",
    "You type in commands and then hit 'Shift Return' to execute them.\n",
    "\n",
    "\n",
    "# Introduction and motivation\n",
    "\n",
    "In the Hardy-Weinberg notebooks, we found that human populations have structure--individuals tend to be slightly more similar to individuals within their group than they are to individuals in other populations. A consequence is that the heterozygosity in a structured population is less than you would expect by chance. This suggests that heterozygosity can be used to measure the degree of differentiation between populations. It can also be used to measure inbreeding. We'll explore these two applications in this notebook.\n",
    "\n",
    "Let's start with what we observed in the Hardy-Weinberg notebook. We defined the excess heterozygosity as \n",
    "\n",
    "$e_{IT}=\\frac{\\mbox{observed proportion of hets}}{\\mbox{expected proportion of hets}}=\\frac{ H_{obs}}{H_{IT}},$\n",
    "\n",
    "\n",
    "where $H_{obs}$ is the observed number of heterozygotes, and $H_{IT}=2 p (1-p) N$ is the expected number of heterozygotes under a random mating assumption (I'll explain the subscript $IT$ in a second). This excess of heterozygotes equals one in a randomly mating population, and zero when there are no heterozygotes (an excess below 1 corresponds to a lack of heterozygotes). \n",
    "\n",
    "Usually, this \"$e$-statistic\" is close to one within species--there is not a vast change in the number of heterozygotes compared to the uniform random-mating assumptions. Because population geneticists are often interested in *departures* from the random-mating assumptions, they usually consider how far $e$ is from 1. This defines the $F$-statistic:\n",
    "\n",
    "$F_{IT}=1-e_{IT}=1-\\frac{H_{obs}}{H_{IT}}$.\n",
    "\n",
    "\n",
    "We found that this $F_{IT}$ is larger than $0.$ So mankind is not a randomly mating populations. But there are (at least) two phenomena that can lead to fewer heterozygotes. You could have population structure, where individuals tend to choose partners within their sub-populations. And we could have inbreeding, where individuals tend to choose partners among their relatives. Population structure and inbreeding have similar effects on global heterozygosity, but there is a way to distinguish the two. When a population is divided into two randomly-mating sub-populations, there should be no departures from Hardy-Weinberg within each sub-population. By contrast, in a case of \"pure\" inbreeding, when individuals just prefer to mate with relatives but there is no additional structure in the population, the lack of heterozygosity should be maintained within the sub-populations. \n",
    "\n",
    "So if we want to look for population structure, we first look at the global loss in heterozygosity, then find how much of it we can blame on within-population shenanigans, and we attribute the rest to population structure. Shenanigans can mean inbreeding, but it can also mean population structure within the sub-population, and other similar complications. It is *not* standard nomenclature. \n",
    "\n",
    "\n",
    "\n",
    "In the Hardy-Weinberg notebook, we computed the excess of heterozygosity in the whole population, which we now call $e_{IT},$ the excess of heterozygotes in individuals ($I$) compared to the expectation based on the entire population $T$. We also computed $e_{shenanigans},$ the excess heterozygosity remaining even after we took into account differences in population frequency. Let's call this $e_{IS},$ the excess heterozygosity in individuals $I$ compared to the expectation based on frequencies in subpopulation $S$. As a reminder, we found that $e_{IT}$ was significantly smaller than one, meaning that $F_{IT}>0.$ We also found that $e_{IS}\\simeq 1,$ so that there was no substantial evidence for shenanigans: $F_{IS}=1-e_{IS}\\simeq 0$.    \n",
    "\n",
    "This suggests that the lack of heterozygocity is indeed due to population stucture. To formalize this, let's say that the frequency of the $A$ allele in the population is $p$, and that this frequency is $p_i$ in subpopulation $i$. We can write the expected number of heterozygotes based on the allele frequency in total population, $H_T=2 p (1-p) N$, where $N$ is the sample size. Similarly, the expectation based on the allele frequencies in each sub-populationis $H_S=\\sum_i 2 N_i p_i (1-p_i),$ where  $N_i$ the sample size of population $i$.  \n",
    "We can divide $e_{IT}$ in two components:\n",
    "\n",
    "$$e_{IT}=\\frac{H_{obs}}{H_T}=\\frac{H_{obs}}{H_S}\\frac{H_S}{H_T}$$\n",
    "\n",
    "The first term on the right-hand side corresponds to $e_{IS}:$ the excess of heterozygotes given population expectations. The second term, then, tells us about the excess of heterozygocity due to differences between subpopulations. We define $e_{ST}=\\frac{H_S}{H_T}$ and $F_{ST}=1-e_{ST}.$ So we have\n",
    "\n",
    "$$e_{IT}=e_{IS}e_{ST}.$$\n",
    "\n",
    "If we express these in terns of $F$-statistics, we get \n",
    "$$1-F_{IT}=\\left(1-F_{IS}\\right)\\left(1-F_{ST}\\right).$$\n",
    "\n",
    "$F_{ST}$ is the most commonly used of the three statistics. To calculate it directly, we use the definition $e_{ST}=\\frac{H_S}{H_T}$ and the expectations for $H_S$ and $H_T$ computed above. \n",
    "\n",
    "The math gets a bit more involved here. Give it a shot, but it's ok if things are a bit confusing until the end of the mathematical interlude.  \n",
    "\n",
    "$$F_{ST}=1-e_{ST}=1-\\frac{\\sum_i N_i p_i (1-p_i)}{N p (1-p)}.$$\n",
    "\n",
    "You can verify that this can be rewritten as\n",
    "\n",
    "$$F_{ST}= \\frac{\\mbox{var}(p_i)}{p (1-p)},$$\n",
    "\n",
    "where $\\mbox{var}(p_i)$ is the variance in the allele frequency between populations, defined as \n",
    "\n",
    "$$\\mbox{var}(p_i) \\equiv E(p_i^2)-E(p_i)^2=\\sum_i \\frac{N_i}{N} p_i^2 -  \\left(\\sum_i \\frac{N_i}{N} p_i\\right)^2.$$\n",
    "\n",
    "\n",
    "This sounds like a pretty clear definition, but there is a lot of different ways to estimate $F_{ST}$ from data. \n",
    "\n",
    "# Reading assigment\n",
    "[This article](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3759727/) provides a nice description of the different issues involved. Please take the time to read it. It's ok if you don't get all the subtleties, but you should be able to describe the general idea.\n",
    "\n",
    "One of the challenges is differences in sample size: If our goal is to partition the missing heterozygosity in a given sample, the definition above is appropriate: $F_{ST}$ will give us the reduction in heterozygosity due to population structure. Unfortunately, this will depend on the sample size from each population, so it won't be super useful to compare across experiments. If we want to use $F_{ST}$ to measure absolute differentiation between two populations, it may be convenient to use equal size for each population $N_i=\\frac{N}{\\#subpopulations}$. These will give different answers, and there is no single \"best\" answer. \n",
    "\n",
    "Here we'll use the sample-size independent definition. For $F_{ST}$ between two populations: \n",
    "\n",
    "$$F_{ST}= \\frac{\\mbox{var}(p_i)}{p (1-p)},$$\n",
    "\n",
    "There are two ways in which we could interpret $\\mbox{var}(p_i)$: as a sample variance, and as a population variance in the allele frequencies. \n",
    "If we interpret $\\mbox{var}(p_i)$ to be the sample variance in allele frequencies, we would get:\n",
    "\n",
    "$$\\begin{split}\n",
    "\\mbox{var}(p_i) \\equiv E(p_i^2)-E(p_i)^2&= \\frac{p_1^2+p_2^2}{2} -  \\left(\\frac{p_1+p_2}{2}\\right)^2.\\\\\n",
    "&=\\frac{(p_1-p_2)^2}{4}.\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Putting the last two equations together, we get:\n",
    "\n",
    "\n",
    "\n",
    "$$F_{ST, \\mbox{sample}}= \\frac{(p_1-p_2)^2}{4 p (1-p)}.$$ \n",
    "\n",
    "\n",
    "Most approaches to estimate $F_{ST}$ interpret $\\mbox{var}(p_i)$ as a *population* variance, where we imagine that the two populations themselves either have evolved from a common ancestral population, or are drawn from an imagined set of populations, all derived from the same ancestral population, and we are trying to estimate the variance of that set of populations relative to the ancestral frequency. In that case, we need to account for the fact that the sample variance will be underestimated by a factor of two, so that the estimate would read: \n",
    "\n",
    "$$F_{ST, \\mbox{population}}= \\frac{(p_1-p_2)^2}{2 p (1-p)}.$$\n",
    "\n",
    "I dislike this interpretation, because it tries to measure the variance of a distribution that does not exist. But it is the most common, and it only differs from the sample definition by a factor of 2. \n",
    "\n",
    "There is another way of thinking about $F_{ST}$, which is hinted at by equation:\n",
    "$$F_{ST}= \\frac{\\mbox{var}(p_i)}{p (1-p)}.$$\n",
    "\n",
    "The numerator is the variance in the allele frequency across populations, and the numerator happens to equal the variance in the genotype if we sample one haploid genotype from the entire population. The numerator tells us about inter-population variance, and the denominator tells us something about the total variance in the genotype. So $F_{ST}$ is telling us something about the proportion of the variance \"due to\" frequency differences across populations. \n",
    "\n",
    "## Wahlund's principle\n",
    "To formalize this, we can use the law of total variance. Let $g$ be the genotype of one particular haploid sample drawn from the entire population ($g=0$ or $1$). The variance $\\mbox{var}(g)$ over all possible samples is simply $p (1-p)$ (since it is a bernoulli trial). The law of total variance is useful to break down the variance of a variable into variance components due to intermediate variables, especially useful when we have a two-step random process. Here, we can imagine that we draw a population first, then draw an allele from that population. the intermediate variable in this case is $i$, the selected population. The law of total variance states that \n",
    " \n",
    "$$\\mbox{var}(g) = \\mbox{var}(E[g|i])+ E[\\mbox{var}[g|i]]  $$\n",
    "The expectation of $g$ knowing that we sampled from population $i$ is $p_i$. The variance of $g$ knowing $i$ is $p_i (1-p_i).$ So the variance of $g$ is \n",
    "\n",
    "$$\\mbox{var}(g) = p (1-p) = \\mbox{var}(p_i)+ E[p_i (1-p_i)].$$\n",
    "\n",
    "Since $F_{ST}= \\frac{\\mbox{var}(p_i)}{p (1-p)},$ we can interpret it as the proportion of genotype variance that can be attributed to differences across populations. The last equation is sometimes referred to as the Wahlund principle. BEcause all three terms are non-negative, it implies that the expected heterozygosity in the subpopulation (the right-most term) is always smaller than expected heterozygosity under Hardy-Weinberg (the left-most term).\n",
    "**(end of the mathematical interlude)**\n",
    "\n",
    "An $F_{ST}$ of zero means that allele frequencies are equal in the subpopulation (hence no detectable population structure), and an $F_{ST, population}$ of two is the maximum possible divergence, which can only happen when $p=0.5$, $p_1=0$, $p_2=1$, or vice versa.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Have a look at the [wikipedia page](http://en.wikipedia.org/wiki/F-statistics) for a different exposition of the same ideas. If you find a better explanation somewhere, let me know!  \n",
    "\n",
    "# Implementation\n",
    "In this notebook we'll want to look at the different $F$-statistics across populations in the 1000 Genomes project. These statistics depend on the number of homozygotes and heterozygotes in each subpopulation. So we'll have to count the number of homozygotes and heterozygotes as in the Hardy-Weinberg example, but we'll have to do that within each population. \n",
    "\n",
    "Fortunately, we already did most of the work in the Hardy-Weinberg notebook. The only difference now is that we'll want to read the SNP for all the populations at the same time.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.1\n",
      "scikit-allel 1.2.1\n"
     ]
    }
   ],
   "source": [
    "### 1\n",
    "import allel\n",
    "import os\n",
    "import gzip\n",
    "print(allel.__version__)\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "sns.set_style('ticks')\n",
    "sns.set_context('notebook')\n",
    "import h5py\n",
    "import allel; print('scikit-allel', allel.__version__)\n",
    "import patsy\n",
    "import statsmodels.api as sm\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and cleaning the data\n",
    "The first step is to import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andjela/anaconda3/lib/python3.7/site-packages/allel/io/vcf_read.py:714: UserWarning: more samples than given in header; field: CALLDATA; variant: 792 (22\u0000:16115695); sample: 2504:0 (unknown:GT)\n",
      "  chunk, _, _, _ = next(it)\n"
     ]
    }
   ],
   "source": [
    "### 2\n",
    "vcf_file_name = 'ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz'\n",
    "\n",
    "vcf_file_path = os.path.join('..',  'data', '1000G', vcf_file_name)\n",
    "h5_file_name = \"chr22.h5\"\n",
    "h5_file_path =  os.path.join('..',  'data', '1000G', h5_file_name)\n",
    "### 4\n",
    "try:\n",
    "    callset = h5py.File(h5_file_path, mode='r')\n",
    "except OSError:\n",
    "    allel.vcf_to_hdf5(vcf_file_path, h5_file_path, fields='*', overwrite=True)\n",
    "    callset = h5py.File(h5_file_path, mode='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter for biallelic loci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3\n",
    "genotypes = allel.GenotypeChunkedArray(callset['calldata/GT'])\n",
    "allele_counts = genotypes.count_alleles()\n",
    "\n",
    "\n",
    "is_biallelic = allele_counts.is_biallelic_01()\n",
    "\n",
    "def report_filter(filter_vector):\n",
    "    kept = np.sum(filter_vector)\n",
    "    length = filter_vector.shape[0]\n",
    "    print(\"kept %d out of %d. Ratio kept: %2.2f \" % (kept, length, kept/length))\n",
    "\n",
    "\n",
    "genotypes_biallelic = genotypes.compress(is_biallelic)\n",
    "pos_biallelic = callset['variants/POS'][:].compress(is_biallelic)\n",
    "report_filter(is_biallelic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract positions in the bed file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4\n",
    "bed_file = os.path.join('..', 'data', '1000G', '20141020.strict_mask.whole_genome.bed')\n",
    "### 39\n",
    "mask_bed = pandas.read_csv(bed_file, sep='\\t', header=None)\n",
    "mask_bed = mask_bed.loc[mask_bed[0] == 'chr22']\n",
    "n_features = mask_bed.shape[0]\n",
    "print(\"number of features:\", n_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5\n",
    "in_mask = (pos_biallelic<0) # Initialize with all False\n",
    "i = 0\n",
    "for _index, feature in mask_bed.iterrows():\n",
    "    i+=1\n",
    "    if i%10000 == 0:\n",
    "        print(\"processed\", i, \"features out of\", n_features)\n",
    "\n",
    "    start = feature[1]\n",
    "    end = feature[2]\n",
    "      \n",
    "    in_mask = np.logical_or(in_mask, np.logical_and(pos_biallelic>=start, \n",
    "                                                    pos_biallelic<=end))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6\n",
    "report_filter(in_mask)\n",
    "strict_genotypes = genotypes_biallelic.compress(in_mask)\n",
    "strict_positions = pos_biallelic.compress(in_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to import the names of the individuals, as in the Hardy-Weinberg notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7\n",
    "population_file_name = 'integrated_call_samples_v3.20130502.ALL.panel'\n",
    "population_file = os.path.join('..', 'data', '1000G', population_file_name)\n",
    "samples = pandas.read_csv(population_file, sep='\\t')\n",
    "samples.head()\n",
    "populations = np.array(samples['pop'].value_counts().keys())\n",
    "\n",
    "samples.reset_index(drop=True, inplace=True)\n",
    "samples.head()\n",
    "\n",
    "subpops = {\n",
    "    # for each population, get the list of samples that belong to the population\n",
    "    pop_iter: samples[samples['pop'] == pop_iter].index.tolist() for pop_iter in populations\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then get some information about the populations and superpopulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8\n",
    "superpops = samples['super_pop'].value_counts().keys()\n",
    "pop_by_superpop = {}\n",
    "continent_by_population = {}\n",
    "for spop in superpops:\n",
    "    is_in_superpop = samples['super_pop'].isin({spop})\n",
    "    subsample = samples[is_in_superpop]\n",
    "    pops = subsample['pop'].value_counts().keys()\n",
    "    pop_by_superpop[spop] = list(pops)\n",
    "    for pop in list(pops):\n",
    "        continent_by_population[pop] = spop    \n",
    "    \n",
    "continentcolor = {'SAS':'r', 'EAS':'b', 'AMR':'orange', 'AFR':'g', 'EUR':'m'}\n",
    "\n",
    "\n",
    "pop_by_superpop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a list of populations sorted by continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 9\n",
    "sorted_pops = [pop for spop in pop_by_superpop.keys() for pop in pop_by_superpop[spop] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing $F_{ST}$ \n",
    " \n",
    "Because F statistics are a staple of population genetics, the scikit-allel package has many functions to compute them (see [this link](https://scikit-allel.readthedocs.io/en/latest/stats/fst.html)). We'll compute Fst with the packaged functions first, but we'll also compute it ourselves. \n",
    "\n",
    "We'll start with the Weir-Cockerham estimator. This approach is described [here](https://www.researchgate.net/publication/200102299_Weir_BS_Cockerham_CC_Estimating_F-Statistics_for_the_Analysis_of_Population-Structure_Evolution_38_1358-1370/download). The article is very technical, so read at your own risk.  \n",
    "\n",
    "Weir and Cockerham define three quantities ($a$, $b$, and $c$), related to Wright's F-statistics\n",
    "\n",
    "$F_{ST} = \\frac{a}{a+b+c}$\n",
    "\n",
    "$F_{IS} = \\frac{b}{b+c}$\n",
    "\n",
    "$F_{IT} = \\frac{a+b}{a+b+c}.$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 10\n",
    "a, b, c = allel.weir_cockerham_fst(strict_genotypes,\n",
    "                                   [subpops[pop] for pop in subpops.keys()], \n",
    "                                   max_allele=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 11\n",
    "fst_WC = a/(a+b+c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Weir-Cockerham estimator is designed to work with arbitrary number of alleles, and reports and Fst for each allele. In the biallelic case, this is not useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 12\n",
    "fst_WC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 13\n",
    "fst_WC = fst_WC[:,0]\n",
    "fst_WC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This provides one estimate of $F_{ST}$ for each variant. If we want to get an average across all SNPs, we could take the average of all these $F_{ST}$ values. One downside of doing this is that high-variance estimates for rare variants will cause problems. An alternative is to take the average of the numerator and divide it by the average of the denominator. These won't be equal to each other. It's not clear at this point which one is best, so we might as well pick the one that has the lowest variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 14\n",
    "ratio_of_means = np.sum(a) / (np.sum(a) + np.sum(b) + np.sum(c))\n",
    "mean_of_ratios = np.mean(fst_WC)\n",
    "print(\"ratio of means\", ratio_of_means)\n",
    "print(\"mean of ratios\", mean_of_ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the two should not be a surprise: large values of $a$ tend to be paired up with large values of $a+b+c$. In the average of ratios, the large $a$s tend to get cancelled more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 15\n",
    "plt.loglog(a[:10000,0],(a+b+c)[:10000,1],'.')\n",
    "plt.xlabel(\"a\")\n",
    "plt.ylabel(\"a+b+c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now try to explore the variation in $F_{ST}$ across the genome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 16\n",
    "plt.plot(strict_positions, fst_WC[:],'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To bin the data, we can choose bins of constant width, or chose segments corresponding to contiguous segments from the mask file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 17\n",
    "windowed_Fst = allel.windowed_weir_cockerham_fst(strict_positions, strict_genotypes, \n",
    "                                  [subpops[pop] for pop in subpops.keys()], size = 1000000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 18\n",
    "windowed_Fst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 19\n",
    "plt.plot(windowed_Fst[1][:,0], windowed_Fst[0],'-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also bin it by contiguous segments in the bed file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 20\n",
    "bed_windows = np.array([mask_bed[1]+1,mask_bed[2]]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 21\n",
    "bed_fst, windows, counts = allel.windowed_weir_cockerham_fst(strict_positions, \n",
    "                        strict_genotypes, [subpops[pop] for pop in subpops.keys()], \n",
    "                        windows=bed_windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can decide to only plot windows with sufficient counts, since windows with few snps will have very variable estimates of $F_{ST}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 22\n",
    "has_enough_snps = (counts > 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 23\n",
    "plt.plot(windows[:,0].compress(has_enough_snps), bed_fst.compress(has_enough_snps),'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not completely straightforward to figure out what is going on here. There is variation in the amount of $F_{ST}$, but it's hard to tell with our current knowledge whether this is due to noise, to genetic drift, or to the action of natural selection. \n",
    "\n",
    "## $F_{ST}$ across populations\n",
    "Instead of comparing $F_{ST}$ for regions along the genome, we might want to compare pairwise $F_{ST}$ across human populations. As discussed in class, there are a few different approaches to compute $F_{ST}$. We'll first consider two approaches used by Hudson and Patterson, because they are already coded in scikit-allel. \n",
    "\n",
    "Notice that both functions return a numerator and a denominator, and that we compute the ratio $$\\frac{E[\\text{num}]}{E[\\text{denom}]},$$ rather than $$E[\\frac{\\text{num}}{\\text{denom}}]$$ \n",
    "This is related to [this paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3759727/), which you should have read by now! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 24\n",
    "hudson = {}\n",
    "hudson_SD = {}\n",
    "patterson = {}\n",
    "patterson_SD = {}\n",
    "nblocks = 1\n",
    "blocklength = (strict_positions[-1]-strict_positions[0])//nblocks\n",
    "print(\"block length = \", blocklength)\n",
    "\n",
    "\n",
    "ac_subpops = strict_genotypes.count_alleles_subpops(subpops, max_allele=1)\n",
    "\n",
    "for p1,p2 in itertools.combinations(sorted_pops,2):\n",
    "\n",
    "    num, den = allel.hudson_fst(ac_subpops[p1], ac_subpops[p2], fill=np.nan)\n",
    "    fst = np.nansum(num) / np.nansum(den)\n",
    "    hudson[(p1,p2)] = fst\n",
    "    \n",
    "    num, den = allel.patterson_fst(ac_subpops[p1], ac_subpops[p2])\n",
    "    fst = np.nansum(num) / np.nansum(den)\n",
    "    patterson[(p1,p2)] = fst\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 25\n",
    "fstarray = np.zeros((len(populations),len(populations)))\n",
    "counter = 0\n",
    "for npop1,pop1 in enumerate(sorted_pops):\n",
    "    for npop2,pop2 in enumerate(sorted_pops):\n",
    "        if (npop1 < npop2):\n",
    "            fstarray[npop1,npop2] = hudson[(pop1,pop2)]\n",
    "        elif (npop1 > npop2):\n",
    "            fstarray[npop1,npop2] = patterson[(pop2,pop1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unless you've spend much time looking at the 1000 Genomes Data, you may not be familiar with the three-letter labels that are used to identify populations, so we'll extract those here. \n",
    "\n",
    "Population descriptions can be found [here](http://ftp-trace.ncbi.nih.gov/1000genomes/ftp/20131219.populations.tsv)\n",
    "We'll record information about the full names of each population label, and the region of each population, so that we can sort populations by regions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 26\n",
    "population_id_file_name =...\n",
    "pandas.read_csv(population_id_file_name,sep = '\\t')[['Population Code',\n",
    "                                                     'Population Description',\n",
    "                                                     'Super Population']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, plot the pairwise $F_{ST}$, with the Hudson estimator in the lower corner and the Patterson in the upper right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 27\n",
    "plt.imshow(fstarray,interpolation='nearest',cmap= 'plasma')\n",
    "plt.colorbar()\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(list(range(len(populations))));\n",
    "ax.set_yticks(list(range(len(populations))));\n",
    "ax.set_xticklabels(sorted_pops,rotation=90);\n",
    "ax.set_yticklabels(sorted_pops);\n",
    "\n",
    "[label.set_color(continentcolor[continent_by_population[sorted_pops[i]]]) \n",
    "    for i,label in enumerate(ax.get_xticklabels())];\n",
    "[label.set_color(continentcolor[continent_by_population[sorted_pops[i]]]) \n",
    "    for i,label in enumerate(ax.get_yticklabels())];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, describe some of your observations--I see patterns of variation both within and across continents. I want at least 4 distinct qualitative observations for which you can come up with plausible explanations.\n",
    "\n",
    "Observations:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One observation is that the two measures of $F_{ST}$ are very consistent. We can measure this explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 28\n",
    "fst_list_patterson = list(patterson.values())\n",
    "fst_list_hudson = list(hudson.values())\n",
    "plt.plot(fst_list_hudson, fst_list_patterson,'.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I find them to be perfectly correlated. Maybe the two measures are actually identical? I have not figured it out yet, but looking at the [source code](https://scikit-allel.readthedocs.io/en/stable/_modules/allel/stats/fst.html#hudson_fst) of scikit-allel.patterson_Fst (there was a TODO there on Jan 14, 2019 suggesting to try to understand whether these two estimators are exactly the same...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's try to write our own $F_{ST}$ calculator. We have the formula\n",
    "\n",
    "$$F_{ST,Sample}= \\frac{(p_1-p_2)^2}{4 p (1-p)}$$\n",
    "\n",
    "\n",
    "This suggests a pretty straightforward way to compute $F_{ST}$: we could estimate the sample frequency in each population $\\hat p_1$ and $\\hat p_2$, and an estimate of the mean frequency, such as $\\hat p = \\frac{\\hat p_1+\\hat p_2}{2}$, and $\\hat F_{ST,sample}= \\frac{(\\hat p_1-\\hat p_2)^2}{4 \\hat p (1-\\hat p)}.$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 29\n",
    "pop1 = \"YRI\"\n",
    "pop2 = \"CEU\"\n",
    "print(\"patterson F_ST, YRI CHB\", patterson[(pop1, pop2)])\n",
    "print(\"hudson F_ST, YRI CHB\", hudson[(pop1, pop2)])\n",
    "\n",
    "ac1 = ac_subpops[pop1]\n",
    "ac2 = ac_subpops[pop2]\n",
    "\n",
    "hat_p1 = ...\n",
    "hat_p2 = ...\n",
    "hat_p ...\n",
    "fst = ...\n",
    "print (\"Our naive FST\", fst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This likely gives you a fairly different estimate. Why do you think that is? The estimate we coded up was initially proposed by Nei (1973) and Nei and Chesser (1983). It was eventually updated by Nei (1986) who added a factor of 2 to get the 'population' $F_{ST}$, as discussed earlier in this notebook. All references can be found in [Bhatia et al](https://www.ncbi.nlm.nih.gov/pubmed/23861382).     \n",
    "\n",
    "This was a bit of a shocker to me: the definition of $F_{ST}$ given in almost all textbooks is different by a factor of two to the one reported by almost all software... \n",
    "\n",
    "As it turns out, the Hudson estimator (Equation 10 in Bhatia et al) can be also be estimated pretty easily. Please try it out in the next cell, and compare the Nei and Hudson estimator on a SNP-by-snp basis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 30\n",
    "### Code the hudson estimator here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More discussion on $F_{ST}$ estimators within scikit-allel can be found [here](http://alimanfoo.github.io/2015/09/21/estimating-fst.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $F_{IS}$\n",
    "\n",
    "Given all these complications around $F_{ST}$, it may come as relief that $F_{IS},$ which measures the excess of homozygosity within populations, is less controversial. We can still compute it as $\\frac{H_{exp}-H_{obs}}{H_{exp}},$ with the expectation computed under Hardy-Weinberg equilibrium. Based on our last notebook, we expect that this will be close to $0$ for most populations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 31\n",
    "\n",
    "# Get biallelic allele counts\n",
    "allele_counts_biallelic_all_alleles = allele_counts.compress(is_biallelic)\n",
    "relevant_column = np.array([False] * allele_counts_biallelic_all_alleles.shape[1])\n",
    "relevant_column[0:2] = True\n",
    "allele_counts_biallelic = allele_counts_biallelic_all_alleles.compress(relevant_column, \n",
    "                                                                       axis = 1)\n",
    "\n",
    "# Count heterozygotes by sample and subpopulation\n",
    "genotypes_012 = genotypes_biallelic.to_n_alt(fill=-1) # n_snps by n_individuals\n",
    "strict_genotypes_012 = genotypes_012.compress(in_mask) # n_snps by n_individuals\n",
    "\n",
    "\n",
    "genotype_012_as_triallelic_haploid = allel.GenotypeArray(strict_genotypes_012[:].reshape(\n",
    "    [strict_genotypes_012.shape[0], strict_genotypes_012.shape[1], 1]))\n",
    "observed_counts_by_subpop = genotype_012_as_triallelic_haploid.count_alleles_subpops(subpops, \n",
    "                                                                           max_allele=2)\n",
    "het_per_sample = (genotype_012_as_triallelic_haploid==1).sum(axis = 0)\n",
    "\n",
    "\n",
    "ac_subpops = strict_genotypes.count_alleles_subpops(subpops, max_allele=1)\n",
    "\n",
    "\n",
    "FIS = []\n",
    "for population in sorted_pops:\n",
    "    allele_counts_subpop = ac_subpops[population]\n",
    "    number_of_genotypes_subpop = observed_counts_by_subpop[population].sum(axis=1)\n",
    "    #calculate expected heterozygosity \n",
    "    het_expectation =...\n",
    "    \n",
    "    #calculate the numerator and denominator of FIS\n",
    "    numerator =...\n",
    "    denominator =...\n",
    "    \n",
    "    #append FIS to FIS list\n",
    "......\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 32\n",
    "plt.plot(FIS,'b',lw=2);\n",
    "plt.xlabel(\"population\")\n",
    "plt.ylabel(\"F_IS\")\n",
    "\n",
    "\n",
    "# I want to color-code the populations by continent and write the name below the graph \n",
    "ax = plt.gca()  # Gives a name to the current plot. \n",
    "ax.set_xticks(list(range(len(populations)))); # puts one tick-mark for each position in \n",
    "                                              # the array\n",
    "ax.set_xticklabels(sorted_pops,rotation=90);  # Gives the name of the population to the tick \n",
    "                                              # mark\n",
    "\n",
    "# Change the color of each tick label according to the continent of origin\n",
    "continentcolor = {'SAS':'r', 'EAS':'b', 'AMR':'orange', 'AFR':'g', 'EUR':'m'}\n",
    "[label.set_color(continentcolor[continent_by_population[sorted_pops[i]]]) \n",
    " for i,label in enumerate(ax.get_xticklabels())];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please interpret your results!\n",
    "In particular, given the population with the highest $F_{IS},$ how much first-cousin matings would we need to explain the level of $F_{IS}$? Is that plausible? Whether plausible or not, what are possible other explanations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright Simon Gravel, McGill University."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
